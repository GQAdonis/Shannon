//! Scientific method workflow strategy.
//!
//! Implements a rigorous scientific approach by composing three cognitive patterns:
//! 1. **Chain of Thought** - Generate hypothesis through structured reasoning
//! 2. **Debate** - Critique hypothesis from multiple perspectives
//! 3. **Research** - Validate with evidence and sources
//!
//! This multi-stage approach provides high-quality analysis with built-in
//! verification and reduces hallucinations through adversarial critique.
//!
//! # Example
//!
//! ```rust,ignore
//! let renderer = PromptRenderer::new()?;
//! let workflow = ScientificWorkflow::new(renderer);
//! let result = workflow.execute(&task).await?;
//! ```

use super::Strategy;
use crate::workflow::prompts::PromptRenderer;
use crate::workflow::task::Task;
use anyhow::{Context, Result};
use async_trait::async_trait;
use serde_json::json;

/// Scientific workflow strategy.
///
/// Composes Chain of Thought â†’ Debate â†’ Research to perform rigorous analysis
/// with hypothesis generation, critique, and evidence-based validation.
///
/// # Architecture
///
/// 1. **Hypothesis Generation (CoT)**: Uses chain of thought reasoning to develop
///    initial hypothesis from the query
/// 2. **Critical Analysis (Debate)**: Examines hypothesis from multiple angles,
///    identifying strengths, weaknesses, and edge cases
/// 3. **Evidence Validation (Research)**: Searches for supporting/contradicting
///    evidence and synthesizes final conclusion
///
/// # Token Budget
///
/// This strategy typically consumes 3-5x tokens compared to simple execution
/// due to multi-stage processing. Budget middleware enforces limits.
#[derive(Debug)]
pub struct ScientificWorkflow {
    /// Prompt renderer for templating.
    renderer: PromptRenderer,
}

impl ScientificWorkflow {
    /// Create a new scientific workflow strategy.
    ///
    /// # Arguments
    ///
    /// * `renderer` - Prompt renderer for generating pattern prompts
    #[must_use]
    pub fn new(renderer: PromptRenderer) -> Self {
        Self { renderer }
    }

    /// Extract context as string from task.
    fn extract_context(task: &Task) -> String {
        if let Some(ctx_array) = task.context.as_array() {
            ctx_array
                .iter()
                .filter_map(|v| v.as_str())
                .collect::<Vec<_>>()
                .join("\n")
        } else if let Some(ctx_str) = task.context.as_str() {
            ctx_str.to_string()
        } else {
            String::new()
        }
    }
}

#[async_trait]
impl Strategy for ScientificWorkflow {
    async fn execute(&self, task: &Task) -> Result<String> {
        tracing::info!(
            task_id = %task.id,
            strategy = "scientific",
            "ðŸ”¬ Starting scientific workflow execution"
        );

        // Stage 1: Chain of Thought (Hypothesis Generation)
        tracing::debug!(task_id = %task.id, stage = "chain_of_thought", "Generating hypothesis");
        let cot_context = json!({
            "query": task.query,
            "context": Self::extract_context(task),
        });
        let cot_prompts = self
            .renderer
            .render("chain_of_thought", &cot_context)
            .context("Failed to render chain_of_thought prompt")?;

        // TODO: Replace with actual LLM call via pattern registry
        // For now, simulate hypothesis generation
        let hypothesis = format!(
            "**Hypothesis (Chain of Thought)**\n\n\
             Based on the query: '{}'\n\n\
             Step 1: Analyzing the core question...\n\
             Step 2: Breaking down into sub-components...\n\
             Step 3: Synthesizing initial hypothesis...\n\n\
             Initial hypothesis: [This is a placeholder. In production, \
             this would be generated by calling an LLM with the rendered prompt]\n\n\
             Prompt tokens: {}, estimated tokens: {}",
            task.query, cot_prompts.metadata.pattern, cot_prompts.metadata.estimated_tokens
        );

        tracing::debug!(
            task_id = %task.id,
            stage = "chain_of_thought",
            tokens = cot_prompts.metadata.estimated_tokens,
            "âœ… Hypothesis generated"
        );

        // Stage 2: Debate (Critique)
        tracing::debug!(task_id = %task.id, stage = "debate", "Performing critical analysis");
        let debate_context = json!({
            "query": task.query,
            "hypothesis": hypothesis,
        });
        let debate_prompts = self
            .renderer
            .render("debate", &debate_context)
            .context("Failed to render debate prompt")?;

        // TODO: Replace with actual LLM call via pattern registry
        let critique = format!(
            "**Critical Analysis (Debate)**\n\n\
             Examining hypothesis from multiple perspectives:\n\n\
             **Perspective 1 (Supportive)**: Strengths and supporting evidence...\n\
             **Perspective 2 (Skeptical)**: Potential weaknesses and limitations...\n\
             **Perspective 3 (Alternative)**: Other approaches or interpretations...\n\n\
             [This is a placeholder. In production, this would be generated by \
             calling an LLM with the rendered debate prompt]\n\n\
             Prompt tokens: {}, estimated tokens: {}",
            debate_prompts.metadata.pattern, debate_prompts.metadata.estimated_tokens
        );

        tracing::debug!(
            task_id = %task.id,
            stage = "debate",
            tokens = debate_prompts.metadata.estimated_tokens,
            "âœ… Critical analysis completed"
        );

        // Stage 3: Research (Validation)
        tracing::debug!(task_id = %task.id, stage = "research", "Validating with evidence");
        let research_context = json!({
            "query": task.query,
            "hypothesis": hypothesis,
            "critique": critique,
        });
        let research_prompts = self
            .renderer
            .render("research", &research_context)
            .context("Failed to render research prompt")?;

        // TODO: Replace with actual LLM call via pattern registry + web search
        let validation = format!(
            "**Evidence-Based Validation (Research)**\n\n\
             Searching for supporting evidence:\n\n\
             **Source 1**: [Placeholder citation]\n\
             **Source 2**: [Placeholder citation]\n\
             **Source 3**: [Placeholder citation]\n\n\
             **Synthesis**: Based on the hypothesis, critique, and available evidence...\n\n\
             [This is a placeholder. In production, this would be generated by \
             calling an LLM with web search results]\n\n\
             Prompt tokens: {}, estimated tokens: {}",
            research_prompts.metadata.pattern, research_prompts.metadata.estimated_tokens
        );

        tracing::debug!(
            task_id = %task.id,
            stage = "research",
            tokens = research_prompts.metadata.estimated_tokens,
            "âœ… Evidence validation completed"
        );

        // Synthesize final result
        let total_estimated_tokens = cot_prompts.metadata.estimated_tokens
            + debate_prompts.metadata.estimated_tokens
            + research_prompts.metadata.estimated_tokens;

        let final_result = format!(
            "# Scientific Analysis\n\n\
             Query: {}\n\n\
             ## Stage 1: Hypothesis Generation\n{}\n\n\
             ## Stage 2: Critical Analysis\n{}\n\n\
             ## Stage 3: Evidence Validation\n{}\n\n\
             ---\n\
             **Workflow Statistics**\n\
             - Total estimated tokens: {}\n\
             - Stages completed: 3/3\n\
             - Strategy: Scientific Method",
            task.query, hypothesis, critique, validation, total_estimated_tokens
        );

        tracing::info!(
            task_id = %task.id,
            strategy = "scientific",
            total_tokens = total_estimated_tokens,
            "ðŸŽ‰ Scientific workflow execution completed"
        );

        Ok(final_result)
    }

    fn name(&self) -> &str {
        "Scientific"
    }

    fn description(&self) -> &str {
        "Rigorous scientific method workflow: generates hypothesis (CoT), \
         performs critical analysis (Debate), and validates with evidence (Research)"
    }

    fn patterns(&self) -> Vec<&str> {
        vec!["chain_of_thought", "debate", "research"]
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::workflow::task::Strategy as TaskStrategy;

    #[tokio::test]
    async fn test_scientific_workflow_execute() {
        let renderer = PromptRenderer::new().unwrap();
        let workflow = ScientificWorkflow::new(renderer);

        let task = Task::new("task-1", "user-1", "What is the impact of climate change?")
            .with_strategy(TaskStrategy::Scientific);

        let result = workflow.execute(&task).await.unwrap();

        // Verify all stages are present in output
        assert!(result.contains("Stage 1: Hypothesis Generation"));
        assert!(result.contains("Stage 2: Critical Analysis"));
        assert!(result.contains("Stage 3: Evidence Validation"));
        assert!(result.contains("Scientific Analysis"));
        assert!(result.contains("Workflow Statistics"));
    }

    #[tokio::test]
    async fn test_scientific_workflow_metadata() {
        let renderer = PromptRenderer::new().unwrap();
        let workflow = ScientificWorkflow::new(renderer);

        assert_eq!(workflow.name(), "Scientific");
        assert_eq!(workflow.patterns().len(), 3);
        assert!(workflow.description().contains("hypothesis"));
    }

    #[tokio::test]
    async fn test_extract_context_array() {
        let task =
            Task::new("task-1", "user-1", "test").with_context(json!(["context1", "context2"]));

        let context = ScientificWorkflow::extract_context(&task);
        assert_eq!(context, "context1\ncontext2");
    }

    #[tokio::test]
    async fn test_extract_context_string() {
        let task = Task::new("task-1", "user-1", "test").with_context(json!("single context"));

        let context = ScientificWorkflow::extract_context(&task);
        assert_eq!(context, "single context");
    }

    #[tokio::test]
    async fn test_extract_context_empty() {
        let task = Task::new("task-1", "user-1", "test");

        let context = ScientificWorkflow::extract_context(&task);
        assert!(context.is_empty());
    }
}
