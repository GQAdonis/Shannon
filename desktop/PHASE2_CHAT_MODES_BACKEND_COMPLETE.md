# Phase 2: Dual-Mode Chat System - Backend Implementation âœ…

**Status**: Backend Implementation Complete  
**Date**: 2026-01-12  
**Task Reference**: `plans/shannon-desktop-cherry-parity-plan.md` Phase 2

## Overview

Successfully implemented a dual-mode chat system backend in Rust for the Tauri desktop application. The system provides two distinct chat modes:

- **Quick Chat**: Fast, conversational, direct LLM calls (<500ms target)
- **Task Chat**: Complex, workflow-based, durable Shannon execution

## Implementation Summary

### âœ… Completed Components

#### 1. Module Structure (`desktop/src-tauri/src/chat_modes/`)

Created modular architecture with clear separation of concerns:

```
chat_modes/
â”œâ”€â”€ mod.rs         # Core types and enums (ChatMode, ChatMessage, configs)
â”œâ”€â”€ quick.rs       # QuickChatService (direct LLM provider calls)
â”œâ”€â”€ task.rs        # TaskChatService (Shannon API integration)
â””â”€â”€ detector.rs    # ModeDetector (automatic mode selection)
```

#### 2. Quick Chat Service (`quick.rs`)

**Features Implemented:**
- Direct HTTP calls to LLM providers (OpenAI, Anthropic, Google)
- Streaming support via Server-Sent Events (SSE)
- Multi-provider support with unified interface
- Automatic API key resolution from environment
- Target latency: <500ms for first token

**Supported Providers:**
- OpenAI (GPT-4, GPT-3.5-turbo)
- Anthropic (Claude 3.5, Claude 3)
- Google (Gemini Pro, Gemini Flash)

**Key Methods:**
- `send_message()` - Main entry point for chat
- `call_openai()` - OpenAI API integration
- `call_anthropic()` - Anthropic API integration  
- `call_google()` - Google Gemini integration
- Stream parsing for each provider

#### 3. Task Chat Service (`task.rs`)

**Features Implemented:**
- Integration with Shannon API workflow engine
- Strategy selection based on complexity
- SSE streaming for progress updates
- Durable execution with resumability
- Task lifecycle management (submit, status, cancel)

**Workflow Strategies:**
- Simple: Chain of Thought
- Complex: Scientific Method
- Exploratory: Tree of Thoughts

**Key Methods:**
- `submit_task()` - Submit to Shannon API
- `stream_updates()` - Real-time progress via SSE
- `get_task_status()` - Query task state
- `cancel_task()` - Cancel running tasks

#### 4. Mode Detector (`detector.rs`)

**Detection Strategy:**
- Keyword analysis (quick vs complexity markers)
- Query length heuristics (>500 words â†’ Task mode)
- Intent classification (conversational vs analytical)
- Multi-step instruction detection
- Code generation markers
- Default to Quick mode for ambiguous cases

**Analysis Features:**
- Confidence scoring (0.0-1.0)
- Detailed query analysis for debugging
- Comprehensive test coverage

**Test Coverage:**
- Simple questions â†’ Quick mode
- Conversational queries â†’ Quick mode
- Research queries â†’ Task mode
- Analytical queries â†’ Task mode
- Complex multi-step â†’ Task mode
- Code generation â†’ Task mode

#### 5. Tauri Commands (`lib.rs`)

**Registered Commands:**
```rust
// Chat mode detection
detect_chat_mode(query: String) -> ChatMode

// Quick chat
quick_chat(
    message: String,
    history: Vec<ChatMessage>,
    config: QuickChatConfig
) -> Vec<String>

// Task chat
submit_task_chat(
    query: String,
    context: Vec<String>,
    config: TaskChatConfig,
    state: TauriEmbeddedState
) -> TaskHandle

get_task_status(
    task_id: String,
    state: TauriEmbeddedState
) -> TaskHandle

cancel_task(
    task_id: String,
    state: TauriEmbeddedState
) -> ()
```

### ðŸ“¦ Dependencies Added

#### `desktop/src-tauri/Cargo.toml`
```toml
reqwest = { workspace = true, features = ["stream"], optional = true }
tokio-stream = { version = "0.1", optional = true }
```

Added to `desktop` feature:
- `dep:tokio-stream` - For async stream handling

#### `rust/shannon-api/Cargo.toml`
```toml
regex = "1.11"  # For workflow export pattern matching
```

## Architecture

### Quick Chat Flow
```text
User Query
    â†“
QuickChatService
    â†“
[OpenAI | Anthropic | Google] API
    â†“
SSE Stream â†’ Frontend
```

### Task Chat Flow
```text
User Query
    â†“
ModeDetector (analyzes complexity)
    â†“
TaskChatService
    â†“
Shannon API (/api/tasks)
    â†“
Workflow Engine (durable-shannon)
    â†“
SSE Stream â†’ Frontend
```

## Type Definitions

### Core Types

```rust
pub enum ChatMode {
    Quick,    // Direct LLM
    Task,     // Workflow-based
}

pub struct ChatMessage {
    pub role: String,        // "user" | "assistant" | "system"
    pub content: String,
    pub timestamp: String,   // ISO 8601
}

pub struct QuickChatConfig {
    pub provider: String,    // "openai" | "anthropic" | "google"
    pub model: String,
    pub temperature: f32,
    pub max_tokens: u32,
    pub stream: bool,
}

pub struct TaskChatConfig {
    pub strategy: String,    // "chain_of_thought" | "scientific" | "exploratory"
    pub require_approval: bool,
    pub max_agents: u32,
    pub token_budget: u32,
    pub complexity: TaskComplexity,
}

pub enum TaskComplexity {
    Simple,       // Chain of Thought
    Complex,      // Scientific method
    Exploratory,  // Tree of Thoughts
}

pub struct TaskHandle {
    pub task_id: String,
    pub status: String,
    pub created_at: String,
}

pub struct TaskEvent {
    pub event_type: String,  // "progress" | "tool_call" | "completion" | "error"
    pub payload: serde_json::Value,
    pub timestamp: String,
}
```

## Build Verification

âœ… **Successful Build**
```bash
cd desktop/src-tauri
cargo check --features desktop
# Exit code: 0 (Success)
# Only minor warnings, no errors
```

## Testing Strategy

### Unit Tests Included

1. **ModeDetector Tests** (`detector.rs`)
   - âœ… Simple questions use Quick mode
   - âœ… Conversational queries use Quick mode
   - âœ… Research queries use Task mode
   - âœ… Analytical queries use Task mode
   - âœ… Complex queries use Task mode
   - âœ… Multi-step queries use Task mode
   - âœ… Code generation uses Task mode
   - âœ… Long queries (>500 words) use Task mode
   - âœ… Confidence scoring works
   - âœ… Query analysis provides insights

2. **QuickChatService Tests** (`quick.rs`)
   - âœ… Service creation
   - ðŸ”„ OpenAI call (requires API key)

3. **TaskChatService Tests** (`task.rs`)
   - âœ… Service creation
   - âœ… Strategy selection logic
   - âœ… Explicit strategy override
   - ðŸ”„ Task submission (requires Shannon API)

### Integration Tests (Pending Frontend)

These will be validated once the frontend is implemented:

- [ ] Test quick chat with simple query ("What is 2+2?")
- [ ] Test task chat with complex query ("Research quantum computing")
- [ ] Test mode detection with various queries
- [ ] Verify streaming for both modes
- [ ] Measure and optimize quick chat latency (<500ms target)

## Frontend Integration

### Next Steps

The backend is ready for frontend integration. The frontend team should:

1. **Import Tauri Commands**
   ```typescript
   import { invoke } from '@tauri-apps/api/core';
   
   // Detect mode
   const mode = await invoke<'quick' | 'task'>('detect_chat_mode', {
     query: userMessage
   });
   
   // Quick chat
   const chunks = await invoke<string[]>('quick_chat', {
     message: userMessage,
     history: chatHistory,
     config: {
       provider: 'openai',
       model: 'gpt-4',
       temperature: 0.7,
       max_tokens: 2048,
       stream: true
     }
   });
   
   // Task chat
   const taskHandle = await invoke<TaskHandle>('submit_task_chat', {
     query: userMessage,
     context: [],
     config: {
       strategy: 'auto',
       require_approval: false,
       max_agents: 5,
       token_budget: 50000,
       complexity: 'simple'
     }
   });
   ```

2. **Create TypeScript Services**
   - `QuickChatService` (TypeScript wrapper)
   - `TaskChatService` (TypeScript wrapper)
   - Mode selector UI component
   - Chat UI components with mode indicator

3. **Environment Setup**
   - Ensure API keys are set:
     ```bash
     OPENAI_API_KEY=sk-...
     ANTHROPIC_API_KEY=sk-ant-...
     GOOGLE_API_KEY=...
     ```

## Success Criteria

### âœ… Completed

- [x] Chat modes module structure created
- [x] Quick chat sends direct LLM requests
- [x] Task chat submits to Shannon API
- [x] Mode detector classifies queries correctly
- [x] Tauri commands expose functionality
- [x] Build successful without errors
- [x] Unit tests pass for mode detection

### ðŸ”„ Pending (Frontend Phase)

- [ ] Streaming works for both modes (needs frontend)
- [ ] <500ms latency for quick chat (needs performance testing)
- [ ] End-to-end testing with real queries

## Performance Considerations

### Quick Chat Optimizations
- Direct API calls bypass workflow overhead
- Streaming reduces perceived latency
- Connection pooling via reqwest client
- Target: <500ms for first token

### Task Chat Optimizations
- Asynchronous workflow execution
- SSE for real-time progress updates
- Durable execution allows resume after interruption
- Parallel tool execution in workflow engine

## Error Handling

### Comprehensive Error Management
- Provider-specific error messages
- Network timeout handling (60s for quick, 300s for task)
- Graceful fallback when Shannon API unavailable
- User-friendly error messages in Tauri commands

## Security

### API Key Management
- Keys loaded from environment variables
- Never exposed to frontend
- Secure transmission to LLM providers
- Shannon API embedded mode uses local authentication

## Documentation

### Rust Documentation
All modules include comprehensive rustdoc comments:
- Module-level documentation
- Function-level documentation with examples
- Error documentation
- Safety considerations

### Code Examples
See unit tests for usage examples of each service.

## Related Files

### Created
- `desktop/src-tauri/src/chat_modes/mod.rs`
- `desktop/src-tauri/src/chat_modes/quick.rs`
- `desktop/src-tauri/src/chat_modes/task.rs`
- `desktop/src-tauri/src/chat_modes/detector.rs`

### Modified
- `desktop/src-tauri/src/lib.rs` - Added module and commands
- `desktop/src-tauri/Cargo.toml` - Added dependencies
- `rust/shannon-api/Cargo.toml` - Added regex dependency

## Conclusion

The dual-mode chat system backend is **fully implemented and ready for frontend integration**. The architecture provides a solid foundation for both fast conversational chat and complex workflow-based tasks, with automatic mode detection to provide the best user experience.

The implementation follows Rust best practices, includes comprehensive error handling, and maintains clean separation of concerns between the two chat modes.

**Next Phase**: Frontend implementation of chat UI and TypeScript service wrappers.
